{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e77166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading \u001b[1mcuda/12.1\u001b[22m\u001b[m\n",
      "  \u001b[94mLoading requirement\u001b[0m: cudnn/8.9.1-cu12.x\u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "#!source ../sel-env/bin/activate\n",
    "#!module load cuda/12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets evaluate accelerate torch torchvision torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de402d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402df2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/tsuehr/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa3b78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "softmax = torch.nn.Softmax(-1)\n",
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd3c3f",
   "metadata": {},
   "source": [
    "## Tokens and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "989decdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\")\n",
    "\n",
    "print(len(tokenizer))\n",
    "tokenizer.encode(\"hello, my name is tom\")\n",
    "print(tokenizer.decode(22172))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39091e01",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16a44c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I love natural language processing\n",
      "Tokens: tensor([[ 101, 1045, 2293, 3019, 2653, 6364,  102]])\n",
      "Shape of embeddings: torch.Size([1, 7, 768])\n",
      "Embeddings: tensor([[[-0.0264,  0.2223, -0.1408,  ..., -0.0352,  0.1298,  0.3851],\n",
      "         [ 0.4655,  0.2747, -0.4151,  ..., -0.0558,  0.3812,  0.3176],\n",
      "         [ 0.9949,  1.1442,  0.1125,  ...,  0.0761,  0.2811,  0.3166],\n",
      "         ...,\n",
      "         [-0.2208,  0.4178, -0.2743,  ..., -0.7288, -0.0709,  0.1915],\n",
      "         [-0.2161, -0.3914, -0.3935,  ...,  0.7627,  0.1068, -0.0741],\n",
      "         [ 0.8094,  0.2168, -0.3047,  ...,  0.2330, -0.7237, -0.1818]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import logging\n",
    "\n",
    "# Set the logging level to ERROR to avoid seeing warnings\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Load a pretrained BERT model and its tokenizer from Hugging Face\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example sentence to encode\n",
    "sentence = \"I love natural language processing\"\n",
    "\n",
    "# Tokenize the input sentence\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "print(f\"Prompt: {sentence}\")\n",
    "print(f\"Tokens: {inputs.input_ids}\")\n",
    "\n",
    "# Forward pass through the model to get the hidden states (embeddings)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The embeddings are in the 'last_hidden_state' output\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Show the embeddings for the tokens in the sentence\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "print(\"Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50c52807",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "#We can use this if we want the embeddings of our model as output\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "\n",
    "#or meta-llama/Llama-3.2-1B\n",
    "#or meta-llama/Llama-3.1-8B-Instruct\n",
    "#or \"gpt2\"\n",
    "#or \"TinyLlama/TinyLlama_v1.1\"\n",
    "\n",
    "def get_logprob(model,tokenizer,prompt, completion):\n",
    "    # Encode both the prompt and the prompt + completion\n",
    "    # We need to encode the prompt to know how many tokens to mask out\n",
    "    prompt_tokenized = tokenizer.encode(prompt)\n",
    "    print(prompt)\n",
    "    print(prompt_tokenized)\n",
    "    prompt_completion_tokenized = tokenizer.encode(prompt + completion)\n",
    "    print(prompt + completion)\n",
    "    print(prompt_completion_tokenized)\n",
    "    \n",
    "    # The input_ids are simply the tokenized prompt + completion\n",
    "    input_ids = torch.tensor([prompt_completion_tokenized]) # = vector in pytorch\n",
    "    print(input_ids)\n",
    "\n",
    "    # The labels are the same as the input_ids, but with the prompt tokens masked out\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # mask the sequence (= setting tokens to -100) where we do not want got the probability\n",
    "    print(\"labels:\", labels)\n",
    "    labels[:, :len(prompt_tokenized)] = -100 # cross entropy loss ignores labels set to -100 (we mask the prompt)\n",
    "    print(\"labels masked:\", labels)\n",
    "\n",
    "    # Pass the input_ids and labels to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels) # to get log probabilities\n",
    "    \n",
    "    # Extract the log probability from the model output (e.g., the loss)\n",
    "    loss = outputs.loss.item()\n",
    "    logprob = -loss # loss is equal to log probability\n",
    "    return logprob\n",
    "\n",
    "def get_next_word(model,tokenizer,prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')  # Return as a tensor\n",
    "\n",
    "    # Generate the next token\n",
    "    with torch.no_grad():\n",
    "        #For text generation, we use the .generate() method\n",
    "        outputs = model.generate(input_ids, max_new_tokens=1, do_sample=False)  # Predict the next token\n",
    "        #> if you want to sample from the token distribution; if True = not always get the same token\n",
    "        # outputs = model.generate(input_ids, max_new_tokens=1, do_sample=True, temperature=0.7)\n",
    "        #> higher temperature more equally distributed distribution, if 0 = deterministic\n",
    "\n",
    "    # Decode the output to get the predicted next word\n",
    "    print(outputs[0])\n",
    "    next_word = tokenizer.decode(outputs[0][-1], skip_special_tokens=False)\n",
    "\n",
    "    return next_word\n",
    "\n",
    "def get_answer_option_probabilities(model,tokenizer,prompt,options):\n",
    "    # Encode both the prompt and the prompt + completion\n",
    "    # We need to encode the prompt to know how many tokens to mask out\n",
    "    prompt_tokenized = tokenizer.encode(prompt)\n",
    "    \n",
    "    # The input_ids are simply the tokenized prompt + completion\n",
    "    input_ids = torch.tensor([prompt_tokenized])\n",
    "    # The labels are the same as the input_ids, but with the prompt tokens masked out\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # Pass the input_ids and labels to the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        \n",
    "    logits = outputs.logits[:,-1,:][0] # output.logits has the shape (batch_size, sequence_length, vocab_size)\n",
    "    \n",
    "    option_token_ids = np.array([i[-1] for i in tokenizer(list(options))[\"input_ids\"]])\n",
    "    probs = softmax(logits)[option_token_ids]\n",
    "    print(options)\n",
    "    print(probs)\n",
    "    print(f\"Most likely answer is: {options[torch.argmax(probs)]}\")\n",
    "#     predicted_token_id = torch.argmax(logits, dim=-1)\n",
    "#     print(tokenizer.decode(predicted_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The mouse eat the\"\n",
    "completions = [\" Madrid\", \" Barcelona\", \" Paris\", \" Rome\"]\n",
    "completion = \" Madrid\"\n",
    "\n",
    "logprob = get_logprob(model,tokenizer,prompt,completion)\n",
    "print(f\"Log probability of '{completion}' given ´´{prompt}´´: {logprob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4625f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Spain is\n",
      "[464, 3139, 286, 8602, 318]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m completions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Madrid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Barcelona\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Paris\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Rome\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Madrid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m logprob \u001b[38;5;241m=\u001b[39m get_logprob(model,tokenizer,prompt,completions)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLog probability of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompletions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m given ´´\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m´´: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogprob\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m, in \u001b[0;36mget_logprob\u001b[1;34m(model, tokenizer, prompt, completion)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt_tokenized)\n\u001b[1;32m---> 18\u001b[0m prompt_completion_tokenized \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt \u001b[38;5;241m+\u001b[39m completion)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt \u001b[38;5;241m+\u001b[39m completion)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt_completion_tokenized)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of Spain is\"\n",
    "completions = [\" Madrid\", \" Barcelona\", \" Paris\", \" Rome\"]\n",
    "completion = \" Madrid\"\n",
    "\n",
    "logprob = get_logprob(model,tokenizer,prompt,completion)\n",
    "print(f\"Log probability of '{completion}' given ´´{prompt}´´: {logprob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a2e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  464,  3139,   286,  8602,   318, 14708])\n",
      "The predicted next word is : Madrid\n"
     ]
    }
   ],
   "source": [
    "next_word = get_next_word(model,tokenizer,prompt)\n",
    "print(f\"The predicted next word is :{next_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee4139",
   "metadata": {},
   "source": [
    "### MC-Question Answering (MCQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d222ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D']\n",
      "tensor([1.3917e-05, 2.7317e-06, 6.7857e-06, 1.6165e-06])\n",
      "Most likely answer is: A\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of Spain?\" + '\\n' + 'A:Madrid, B:Barcelona, C:Paris, D: Rome' + '\\nAnswer'\n",
    "options= ['A','B','C','D']\n",
    "get_answer_option_probabilities(model,tokenizer,prompt,options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a3bcd",
   "metadata": {},
   "source": [
    "### A look inside Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b253db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb\n",
    "\n",
    "# identical to slide 23!\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att =  GroupedQueryAttention( # newer form of multi-head attention\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
    "            rope_base=cfg[\"rope_base\"],\n",
    "            rope_config=cfg[\"rope_freq\"],\n",
    "            dtype=cfg[\"dtype\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg) # linear transformation\n",
    "        self.norm1 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5) # normalization function\n",
    "        self.norm2 = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x.to(torch.bfloat16))   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x.to(torch.bfloat16)) # linear tranformation\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Llama3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"]) # 1. embedding\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) # transformers bock are sequentially connected\n",
    "\n",
    "        self.final_norm = nn.RMSNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(torch.bfloat16))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b47fa2",
   "metadata": {},
   "source": [
    "### Pseudocode for supervised learning with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pseudocode for supervised learning\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for x, y in data:\n",
    "\n",
    "#         # Forward pass\n",
    "#         prediction = model(x)\n",
    "\n",
    "#         # Calculate loss\n",
    "#         loss = criterion(prediction, y) # e.g. binary cross entropy loss\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc402ba",
   "metadata": {},
   "source": [
    "### Pseudocode for Reinforcement Learning with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed8acb63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'environment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m----> 4\u001b[0m     state \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mreset() \u001b[38;5;66;03m#reset the world\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(update_steps):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# sample an action according to the current policy\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# sampling an action is like sampling a token\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'environment' is not defined"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = environment.reset() #reset the world\n",
    "    episode_reward = 0\n",
    "    for t in range(update_steps):\n",
    "        # sample an action according to the current policy\n",
    "        # sampling an action is like sampling a token\n",
    "        action, log_prob = ppo_agent.policy.act(state)\n",
    "        \n",
    "        # take action and get reward and next state\n",
    "        # the reward is learned as a function of user rankings of different outputs\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        value = ppo_agent.policy.forward(state)[1].item()\n",
    "\n",
    "        # Store transition\n",
    "        ppo_agent.store_transition((state, action, reward, done, log_prob.item(), value))\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    # Update PPO after every episode\n",
    "    ppo_agent.update()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee3a36",
   "metadata": {},
   "source": [
    "### Fine-Tuning with axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c8e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/axolotl-ai-cloud/axolotl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f359d70",
   "metadata": {},
   "source": [
    "### Fine-Tuning with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "496015c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_Model:\n",
    "    def __init__(self, model_name='gpt2', **kwargs):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True, **kwargs)\n",
    "        self.max_length = self.tokenizer.model_max_length\n",
    "\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        \"\"\" Obtain the vector embedding of a text by taking the hidden state of the last layer of the model\n",
    "        corresponding to the last token.\n",
    "\n",
    "        Inputs:\n",
    "        - text: a string\n",
    "\n",
    "        Returns:\n",
    "        - embedding: a numpy array of shape (n,), where n is the dimensionality of the embedding\n",
    "        \"\"\"\n",
    "\n",
    "        # tokenize\n",
    "        input_ids = self.tokenizer(text).input_ids\n",
    "\n",
    "        # truncate from the left such that we keep the QA template if there is one\n",
    "        input_ids = input_ids[-min(len(input_ids), self.max_length):]\n",
    "\n",
    "        # convert to torch tensor and add batch dimension\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0).to(self.model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids)\n",
    "\n",
    "        last_layer = outputs['hidden_states'][-1]\n",
    "        last_layer_last_token = last_layer[0, -1]\n",
    "        embedding = last_layer_last_token.cpu().double().numpy()\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "def clopper_pearson(n_correct, n_trials, alpha=0.05):\n",
    "    lower_ci = beta.ppf(alpha / 2, n_correct, n_trials - n_correct + 1)\n",
    "    upper_ci = beta.ppf(1 - alpha / 2, n_correct + 1, n_trials - n_correct)\n",
    "    return lower_ci, upper_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a718e4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Embedding_Model(model_name='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aba4a9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3479b4f1cb4ac4b6b1b05b30b6d050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/51.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fenn\\.cache\\huggingface\\hub\\datasets--ricdomolm--lawma-tasks. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f9d785437247ad967deb9ed062c712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f4f41323404de6bc6af56e933096a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val-00000-of-00001.parquet:   0%|          | 0.00/12.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f211e7e23d4793951de81a04bf7dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/25.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dfe1be640b4a7ca5d4a46c4ef74ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c72d4c771647c4a903183cf52641f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81d38c34b79405396d1b63469e7285f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1773 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = datasets.load_dataset('ricdomolm/lawma-tasks', 'sc_issuearea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba51650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example court opinion:\n",
      "----------------\n",
      "ALABAMA et al. v. PUGH et al.\n",
      "No. 77-1107.\n",
      "Decided July 3, 1978\n",
      "Per Curiam.\n",
      "Respondents, inmates or former inmates of the Alabama prison system, sued petitioners, who include the State of Alabama and the Alabama Board of Corrections as well as a number of Alabama officials responsible for the administration of its prisons, alleging that conditions in Alabama prisons constituted cruel and unusual punishment in violation of the Eighth and Fourteenth Amendments. The United States District Court agreed and issued an order prescribing measures designed to eradicate cruel and unusual punishment in the Alabama prison system. The Court of Appeals for the Fifth Circuit affirmed but modified some aspects of the order which it believed exceeded the limits of the appropriate exercise of the court's remedial powers. 559 F. 2d 283.\n",
      "Among the claims raised here by petitioners is that the issuance of a mandatory injunction against the State of Alabama and the Alabama Board of Corrections is unconstitutional because the Eleventh Amendment prohibits federal courts from entertaining suits by private parties against States and their agencies. The Court of Appeals did not address this contention, perhaps because it was of the view that in light of the numerous individual defendants in the case dismissal as to these two defendants would not affect the scope of the injunction. There can be no doubt, however, that suit against the State and its Board of Corrections is barred by the Eleventh Amendment, unless Alabama has consented to the filing of such a suit. Edelman v. Jordan, 415 U. S. 651 (1974); Ford Motor Co. v. Department of Treasury, 323 U. S. 459 (1945); Worcester County Trust Co. v. Riley, 302 U. S. 292 (1937). Respondents do not contend that Alabama has consented to this suit, and it appears that no consent could be given under Art. I, § 14, of the Alabama Constitution, which provides that “the State of Alabama shall never be made a defendant in any court of law or equity.” Moreover, the question of the State’s Eleventh Amendment immunity is not merely academic. Alabama has an interest in being dismissed from this action in order to eliminate the danger of being held, in contempt if it should fail to comply with the mandatory injunction. Consequently, we grant the petition for certio-rari limited to Question 2 presented by petitioners, reverse the judgment in part, and remand the case to the Court of Appeals with instructions to order the dismissal of the State of Alabama and the Alabama Board of Corrections from this action.\n",
      "So ordered.\n",
      "Mr. Justice Brennan and Mr. Justice Marshall dissent.\n",
      "Respondents contend that petitioners failed to raise the Eleventh Amendment issue in the District Court. The Court held in Edelman v. Jordan, 415 U. S. 651, 678 (1974), however, that “the Eleventh Amendment defense sufficiently partakes of the nature of a jurisdictional bar so that it need not be raised in the trial court . . . .”\n",
      "“Whether the mandatory injunction issued against the State of Alabama and the Alabama Board of Corrections violates the State’s Eleventh Amendment immunity or exceeds the jurisdiction granted federal courts by 42 U. S. C. § 1983.”\n",
      "----------------\n",
      "\n",
      "\n",
      "Question: What is the issue area of the decision?\n",
      "----------------\n",
      "Classes: ['Criminal Procedure', 'Civil Rights', 'First Amendment', 'Due Process', 'Privacy', 'Attorneys', 'Unions', 'Economic Activity', 'Judicial Power', 'Federalism', 'Interstate Relations', 'Federal Taxation', 'Miscellaneous', 'Private Action']\n"
     ]
    }
   ],
   "source": [
    "#custom data format is\n",
    "#{opinion:\"...\", question: \"...\", choices:[...]}\n",
    "\n",
    "# print example\n",
    "print('Example court opinion:')\n",
    "print(\"----------------\")\n",
    "print(task['train'][0]['opinion'])\n",
    "print(\"----------------\")\n",
    "print(\"\\n\\nQuestion:\", task['train'][0]['question'])\n",
    "print(\"----------------\")\n",
    "print(\"Classes:\", task['train'][0]['choices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8e4e42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cda8871e5e84a599726f27f1773e1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95f0276da634c0da303e309d20d8ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# subsample the training and test sets to make training and evaluation faster\n",
    "train_set = task['train'].shuffle(seed=1).select(range(500))\n",
    "test_set = task['test'].shuffle(seed=1).select(range(200))\n",
    "\n",
    "# only include the first 3500 characters of the opinion, to make training faster\n",
    "train_set = train_set.map(lambda x: {'opinion': x['opinion'][:3500]})\n",
    "test_set = test_set.map(lambda x: {'opinion': x['opinion'][:3500]})\n",
    "\n",
    "# get the labels of each example\n",
    "train_labels = [example['answer'][0] for example in train_set]\n",
    "test_labels = [example['answer'][0] for example in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeb64265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [10:59<00:00,  1.32s/it]\n",
      "100%|██████████| 200/200 [04:27<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = np.stack([model.get_embedding(example['opinion']) for example in tqdm(train_set)])\n",
    "test_embeddings = np.stack([model.get_embedding(example['opinion']) for example in tqdm(test_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c20399e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the majority class classifier: 0.236\n",
      "Accuracy of our classifier: 0.305 (0.242, 0.374)\n"
     ]
    }
   ],
   "source": [
    "majority_class = max(set(train_labels), key=train_labels.count)\n",
    "accuracy_majority = train_labels.count(majority_class) / len(train_labels)\n",
    "print(f\"Accuracy of the majority class classifier: {accuracy_majority:.3f}\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_performance(train_embeddings, test_embeddings, tol=1e-3):\n",
    "    clf = LogisticRegression(max_iter=100000, tol=tol)\n",
    "\n",
    "    clf = clf.fit(train_embeddings, train_labels)\n",
    "    y_pred = clf.predict(test_embeddings)\n",
    "\n",
    "    n_trials = len(test_labels)\n",
    "    n_correct = sum(y_pred == test_labels)\n",
    "\n",
    "    accuracy = n_correct / n_trials\n",
    "    lower_ci, upper_ci = clopper_pearson(n_correct, n_trials)\n",
    "    return accuracy, (lower_ci, upper_ci)\n",
    "\n",
    "accuracy, (lower_ci, upper_ci) = get_performance(train_embeddings, test_embeddings, tol=1e-4)\n",
    "\n",
    "### Delete everything above\n",
    "\n",
    "print(f\"Accuracy of our classifier: {accuracy:.3f} ({lower_ci:.3f}, {upper_ci:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
